Implement AI Feature Toggles and Unify OpenAI Model

Please implement the following changes in our app to simplify AI usage and control costs:

1) Master AI Toggle

Add a single global flag (AI_ENABLED=true|false) that controls all OpenAI usage.

If disabled â†’ no requests are sent to OpenAI, and endpoints return 503 { code: "AI_DISABLED" }.

2) Per-Feature Toggles

Add flags for each AI feature (server-side enforced):

AI_FEATURE_TRANSLATION (translation not yet implemented, but prepare the toggle now so itâ€™s ready once added)

AI_FEATURE_ASSISTANT

AI_FEATURE_REPORTS

(others as needed)

When a feature flag is off, the API returns 503 { code: "AI_FEATURE_DISABLED" } and the UI hides related functionality.

ðŸ”´ Important: All current and future OpenAI features must go through the gateway and require a toggle (either master or per-feature). No AI usage should exist outside this system.

3) Unify Model Configuration

All OpenAI calls must use the same model: gpt-3.5-turbo.

Remove any hardcoded "gpt-4" or "gpt-5" values from files (e.g., server/aiService.ts).

Centralize configuration so the model is read from OPENAI_MODEL=gpt-3.5-turbo in env.

4) Gateway Module

All OpenAI requests should go through one gateway (server/services/openai/client.ts).

No direct new OpenAI() elsewhere.

Gateway must:

Check toggles before making any request.

Apply consistent error handling and logging.

Enforce timeout and respect rate limits.

5) Admin UI (Settings â†’ AI)

Add a Settings section with:

Master toggle (On/Off).

Feature toggles (Translation / Assistant / Reports).

Display the active model (read-only: gpt-3.5-turbo).

6) Fail-Safe Mechanism

If AI_ENABLED=false or if OPENAI_API_KEY is missing/invalid â†’ automatically disable all AI features, even if individual feature toggles are set to true.

This ensures no OpenAI calls can occur outside of the controlled system.

Acceptance Criteria

Disabling the master flag shuts down all AI endpoints immediately.

Disabling a feature flag only affects that feature.

All AI endpoints and services use only gpt-3.5-turbo.

No hardcoded models remain in the code.

All new AI features added in the future must also register under this system automatically.

Fail-safe is enforced: AI features cannot run without both AI_ENABLED=true and a valid OPENAI_API_KEY.

Documentation updated (README + Admin guide).